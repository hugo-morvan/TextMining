{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you start, make sure that you are familiar with the **[study guide](https://liu-nlp.ai/text-mining/logistics/)**, in particular the rules around **cheating and plagiarism** (found in the course memo).\n",
    "\n",
    "‚û°Ô∏è If you use code from external sources (e.g. StackOverflow, ChatGPT, ...) as part of your solutions, don't forget to add a reference to these source(s) (for example as a comment above your code).\n",
    "\n",
    "‚û°Ô∏è Make sure you fill in all cells that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**.  You normally shouldn't need to modify any of the other cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# L4: Clustering and Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Text clustering groups documents in such a way that documents within a group are more &lsquo;similar&rsquo; to other documents in the cluster than to documents not in the cluster. The exact definition of what &lsquo;similar&rsquo; means in this context varies across applications and clustering algorithms.\n",
    "\n",
    "In this lab you will experiment with both hard and soft clustering techniques. More specifically, in the first part you will be using the $k$-means algorithm, and in the second part you will be using a topic model based on the Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e1945bcb70fbdeb0ecefe1c7930e5b",
     "grade": false,
     "grade_id": "cell-c4776a1666a48ddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define some helper functions that are used in this notebook\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def success():\n",
    "    display(HTML('<div class=\"alert alert-success\"><strong>Checks have passed!</strong></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: Hard clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for the hard clustering part of this lab is a collection of product reviews. We have preprocessed the data by tokenization and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "007298be2cbb2c02ebe809b6a44f0f48",
     "grade": false,
     "grade_id": "cell-59c1ac1d68e6c524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "\n",
    "with bz2.open('reviews.json.bz2') as source:\n",
    "    df = pd.read_json(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the data frame, you can see that there are three labelled columns: `category` (the product category), `sentiment` (whether the product review was classified as &lsquo;positive&rsquo; or &lsquo;negative&rsquo; towards the product), and `text` (the space-separated text of the review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to cluster the product review data using a tf‚Äìidf vectorizer and $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Start by **performing tf‚Äìidf vectorization**. In connection with vectorization, you should also **filter out standard English stop words**. While you could use [spaCy](https://spacy.io/) for this task, here it suffices to use the word list implemented in [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "After running the following cell:\n",
    "- `vectorizer` should contain the vectorizer fitted on `df['text']`\n",
    "- `reviews` should contain the vectorized `df['text']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ffc3d7fdf62fda1c147f978818f56e7",
     "grade": true,
     "grade_id": "cell-66e447662e958a44",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, reviews = ..., ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Next, **write a function to cluster the vectorized data.**  For this, you can use scikit-learn‚Äôs [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class, which has several parameters that you can tweak, the most important one being the _number of clusters_.  Your function should therefore take the number of clusters as an argument; you can leave all other parameters at their defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f795b44fb710deb8ec6557222fe3186f",
     "grade": true,
     "grade_id": "cell-17f7b79dee452c3d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def fit_kmeans(data, n_clusters):\n",
    "    \"\"\"Fit a k-means classifier to some data.\n",
    "\n",
    "    Arguments:\n",
    "        data: The vectorized data to train the classifier on.\n",
    "        n_clusters (int): The number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        The trained k-means classifier.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sanity-check your clustering, **create a bar plot** with the number of documents per cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40b71623600900c4911b5c12c3aa597e",
     "grade": true,
     "grade_id": "cell-d54820cd63b959ee",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cluster_size(kmeans):\n",
    "    \"\"\"Produce & display a bar plot with the number of documents per cluster.\n",
    "\n",
    "    Arguments:\n",
    "        kmeans: The trained k-means classifier.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell shows how your code should run.  The output of the cell should be the bar plot of the cluster sizes.  Note that sizes may vary considerably between clusters and among different random seeds, so there is no single ‚Äúcorrect‚Äù output here!  Re-run the cell a couple of times to observe how the plot changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = fit_kmeans(reviews, 3)\n",
    "plot_cluster_size(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Summarising clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a clustering, you can try to see whether it is meaningful. One useful technique in that context is to **generate a ‚Äúsummary‚Äù** for each cluster by extracting the $n$ highest-weighted terms from the centroid of each cluster. Your next task is to implement this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e9d2fd56a07bcc81a5add0d4df32561",
     "grade": true,
     "grade_id": "cell-163a8b35215df9ad",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cluster_summaries(kmeans, vectorizer, top_n):\n",
    "    \"\"\"Compute the top_n highest-weighted terms from the centroid of each cluster.\n",
    "\n",
    "    Arguments:\n",
    "        kmeans: The trained k-means classifier.\n",
    "        vectorizer: The fitted vectorizer; needed to obtain the actual terms\n",
    "                    belonging to the items in the cluster.\n",
    "        top_n: The number of terms to return for each cluster.\n",
    "\n",
    "    Returns:\n",
    "        A list of length k, where k is the number of clusters. Each item in the list\n",
    "        should be a list of length `top_n` with the highest-weighted terms from that\n",
    "        cluster.  Example:\n",
    "          [[\"first\", \"foo\", ...], [\"second\", \"bar\", ...], [\"third\", \"baz\", ...]]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "The following cell runs your code with `top_n=10` and prints the summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "summaries = compute_cluster_summaries(kmeans, vectorizer, 10)\n",
    "\n",
    "for idx, terms in enumerate(summaries):\n",
    "    print(f\"Cluster {idx}: {', '.join(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have computed the cluster summaries, take a minute to reflect on their quality. Is it clear what the reviews in a given cluster are about? Do the cluster summaries contain any unexpected terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluate clustering performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some scenarios, you may have gold-standard class labels available for at least a subset of your documents.  In our case, we could use the gold-standard categories (from the `category` column) as class labels.  This means we‚Äôre making the assumption that a ‚Äúgood‚Äù clustering should put texts into the same cluster _if and only if_ they belong to the same category.\n",
    "\n",
    "If we have such class labels, we can compute a variety of performance measures to see how well our $k$-means clustering resembles the given class labels.  Here, we will consider three of these measures: the **Rand index (RI)**; the **adjusted Rand index (RI)** which has been corrected for chance; and the **V-measure**.  For all of them (and more), we can make use of [implementations by scikit-learn](https://scikit-learn.org/1.5/modules/clustering.html#clustering-performance-evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to **compare the performance** of different $k$-means clusterings with $k = 1, \\ldots, 10$ clusters.  As your evaluation data, use the _first 1000 documents_ from the original data set along with their gold-standard categories (from the `category` column).\n",
    "\n",
    "**Visualise your results as a line plot**, where\n",
    "\n",
    "- the $x$-axis corresponds to $k$\n",
    "- the $y$-axis corresponds to the score of the evaluation measure\n",
    "- each evaluation measure (RI, ARI, V) is shown by a differently-colored and/or -styled line in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f0d38a480ea2a5e4d7a6d4087498b3f",
     "grade": true,
     "grade_id": "cell-66890753e9a48887",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import rand_score, adjusted_rand_score, v_measure_score\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that you may get different clusters each time you run the $k$-means algorithm, so re-run your solution above a few times to see how the results change.  Take a moment to think how you would interpret these results; you will need this for the reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: Topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for the topic modelling part of this lab is the collection of all [State of the Union](https://en.wikipedia.org/wiki/State_of_the_Union) addresses from the years 1975‚Äì2000. These speeches come as a single text file with one sentence per line. The following code cell prints the first 5 lines from the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with open('sotu_1975_2000.txt') as source:\n",
    "    # Print the first 5 lines only\n",
    "    for line in islice(source, 5):\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes to think about what topics you would expect in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Train a topic model\n",
    "\n",
    "In this problem, we will train an LDA model on the State of the Union&nbsp;(SOTU) dataset. For this, we will be using [spaCy](https://spacy.io/) and the [gensim](https://radimrehurek.com/gensim/) topic modelling library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Preparing the data\n",
    "\n",
    "Start by **preprocessing the data** using spaCy as follows:\n",
    "\n",
    "- Filter out stop words, non-alphabetic tokens, and tokens less than 3 characters in length.\n",
    "- Store the documents as a nested list where the first level of nesting corresponds to the sentences and the second level corresponds to the tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbcdf160b7379a45ddf31b764988d12e",
     "grade": false,
     "grade_id": "cell-201e607b610e47af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_documents(filename=\"sotu_1975_2000.txt\"):\n",
    "    \"\"\"Load and preprocess all documents in the given file.\n",
    "\n",
    "    The preprocessing must filter out stop words, non-alphabetic tokens,\n",
    "    and tokens less than 3 characters in length.\n",
    "\n",
    "    Returns:\n",
    "        A list of length n, where n is the number of documents.\n",
    "        Each item in the list should be a list of tokens in the given\n",
    "        document, after preprocessing.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Test your preprocessing by running the following cell. It will output the tokens (after preprocessing) for an example document and compare them against the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90c9661b2b79a4cf88bdff7e6e6bbfed",
     "grade": true,
     "grade_id": "cell-4fa26bc22c42b359",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "documents = load_and_preprocess_documents()\n",
    "\n",
    "print(f\"Document 42 after preprocessing: {' '.join(documents[42])}\")\n",
    "assert \" \".join(documents[42]) == \"reduce oil imports million barrels day end year million barrels day end\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Training LDA\n",
    "\n",
    "Now that we have the list of documents, we can use gensim to train an LDA model on them.  Gensim works a bit differently from scikit-learn and has its own interfaces, so you should skim the section [‚ÄúPre-process and vectorize the documents‚Äù](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#pre-process-and-vectorize-the-documents) of the documentation to learn how to create the dictionary and the vectorized corpus representation required by gensim.\n",
    "\n",
    "Based on this, **write code to train an [LdaModel](https://radimrehurek.com/gensim/models/ldamodel.html)** for $k=10$ topics, and using default values for all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91f666ca9c35be5c2b905f6ddf334ff4",
     "grade": true,
     "grade_id": "cell-8461457012cd240d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def train_lda_model(documents, num_topics, passes=1):\n",
    "    \"\"\"Create and train an LDA model.\n",
    "\n",
    "    Arguments:\n",
    "        documents: The preprocessed documents, as produced in Task 4.1.\n",
    "        num_topics: The number of topics to generate.\n",
    "        passes: The number of training passes. Defaults to 1; you will need\n",
    "                this later for Task 5.\n",
    "\n",
    "    Returns:\n",
    "        The trained LDA model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "Run the following cell to test your code and print the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_lda_model(documents, 10)\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the topics. Can you &lsquo;label&rsquo; each topic with a short description of what it is about? Do the topics match your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Monitor a topic model for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning an LDA model, it is important to make sure that the training algorithm has converged to a stable posterior distribution. One way to do so is to plot, after each training epochs (or &lsquo;pass&rsquo;, in gensim parlance) the log likelihood of the training data under the posterior. Your last task in this lab is to create such a plot and, based on this, to suggest an appropriate number of epochs.\n",
    "\n",
    "To collect information about the posterior likelihood after each pass, we need to enable the logging facilities of gensim. Once this is done, gensim will add various diagnostics to a log file `gensim.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='gensim.log', format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "def clear_logfile():\n",
    "    # To empty the log file\n",
    "    with open(\"gensim.log\", \"w\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will parse the generated logfile and return the list of log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_logfile():\n",
    "    \"\"\"Parse gensim.log to extract the log-likelihood scores.\n",
    "\n",
    "    Returns:\n",
    "        A list of log-likelihood scores.\n",
    "    \"\"\"\n",
    "    matcher = re.compile(r'(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity')\n",
    "    likelihoods = []\n",
    "    with open('gensim.log') as source:\n",
    "        for line in source:\n",
    "            match = matcher.search(line)\n",
    "            if match:\n",
    "                likelihoods.append(float(match.group(1)))\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example how to run it ‚Äî note that we call `clear_logfile()` to empty the logfile before training the model. If your code from Problem&nbsp;4 was correct, the result should be a list with a single log-likehood score, since we are doing a single training pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_logfile()\n",
    "model = train_lda_model(documents, 10, passes=1)\n",
    "likelihoods = parse_logfile()\n",
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Plotting log-likelihoods\n",
    "\n",
    "Your task now is to **re-train your LDA model for 50&nbsp;passes**, retrieve the list of log likelihoods, and **create a plot** from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ef26d01bc2b34f2fcde5ed7f9de519f",
     "grade": true,
     "grade_id": "cell-c7b05ddd48f08892",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Interpreting log-likelihoods\n",
    "\n",
    "How do you interpret the plot you produced in Task 5.1? Based on the plot, what would be a reasonable choice for the number of passes? **Retrain your LDA model with that number** and re-inspect the topics it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca1784a4dd4357b0958b6b854d06f209",
     "grade": true,
     "grade_id": "cell-8435567308839ce2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual reflection\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>After you have solved the lab,</strong> write a <em>brief</em> reflection (max. one A4 page) on the question(s) below.  Remember:\n",
    "    <ul>\n",
    "        <li>You are encouraged to discuss this part with your lab partner, but you should each write up your reflection <strong>individually</strong>.</li>\n",
    "        <li><strong>Do not put your answers in the notebook</strong>; upload them in the separate submission opportunity for the reflections on Lisam.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Problem&nbsp;3, you performed an evaluation of $k$-means clustering with different values for $k$.  How do you interpret the results?  What would you expect to be a ‚Äúgood‚Äù number of clusters for this dataset?  What do the evaluation measures suggest would be a ‚Äúgood‚Äù number of clusters?\n",
    "2. How did you choose the number of LDA passes in Task&nbsp;5.2?  Do you consider the topic clusters you got in Task&nbsp;5.2 to be ‚Äúbetter‚Äù than the ones from Task&nbsp;4.2?  Base your reasoning on one or more concrete examples from the LDA output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! üëç**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you submit, **make sure the notebook can be run from start to finish** without errors.  For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"‚è©\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad192d-7557-4cd9-9ead-6699b8de9114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
