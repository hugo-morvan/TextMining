{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "➡️ Before you start, make sure that you are familiar with the **[study guide](https://liu-nlp.ai/text-mining/logistics/)**, in particular the rules around **cheating and plagiarism** (found in the course memo).\n",
    "\n",
    "➡️ If you use code from external sources (e.g. StackOverflow, ChatGPT, ...) as part of your solutions, don't forget to add a reference to these source(s) (for example as a comment above your code).\n",
    "\n",
    "➡️ Make sure you fill in all cells that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**.  You normally shouldn't need to modify any of the other cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use large language models (LLMs) to generate summaries of (short) news articles.  In the first part, you will use an encoder language model to perform extractive text summarization, while in the second part, you will use a decoder LLM to perform abstractive text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_A technical note:_** This lab is made to work without requiring a GPU.  If you have access to a GPU, you will be able to use larger (and better) models than the ones used by default in this notebook, and inference time should be much faster.  In order for this to work, you need to make sure that you have CUDA installed, that [PyTorch is installed for the correct CUDA version](https://pytorch.org/get-started/locally/), and also that you have installed the GPU-enabled version of [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) (which is the default, but not the one used in the lab environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e1945bcb70fbdeb0ecefe1c7930e5b",
     "grade": false,
     "grade_id": "cell-c4776a1666a48ddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define some helper functions that are used in this notebook\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def success():\n",
    "    display(HTML('<div class=\"alert alert-success\"><strong>Checks have passed!</strong></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: DBpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a data set containing a small sample of sentences from DBpedia, each containing the word _record_, but describing different types of entities: either a company, a (music) album, or an athlete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cdbd05d89e466f7a560be0b9c1dd6ea",
     "grade": false,
     "grade_id": "cell-59c1ac1d68e6c524",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "\n",
    "with bz2.open(\"dbpedia_record_sample.json.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two labelled columns in the data set: `sentence` (the sentence from DBpedia), and `label` (the category of the DBpedia entry where the sentence is from).  The `label` column can take three values: Company, Album, or Athlete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (_or_ Problem 0): Sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural language models can be used to produce _embeddings_ of words, sentences, or entire documents — i.e., a representation of that string of text in a large-dimensional vector space.\n",
    "\n",
    "In this lab, we are interested in _sentence embeddings_. The library [SentenceTransformers](https://www.sbert.net/) is specifically made for embedding sentences with neural, transformer-based models.  We start by importing this library and loading a small, pre-trained language model.\n",
    "\n",
    "**_Note:_** The first time you run this, the model will automatically be downloaded onto your machine (ca. 90 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s very easy to obtain sentence embeddings from a loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "model.encode(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result of calling `model.encode()` is an array of vectors of floating-point numbers.  Let’s check the _dimensionality_ of these embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to simply **encode all sentences from the DBpedia dataset** using this embedding model, and also produce a list of the corresponding _labels_ for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2f8e7ff05cac9e8b805b8a8a112666f",
     "grade": true,
     "grade_id": "cell-de8a785e8a4c9a09",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "embeddings = ...\n",
    "labels = []       # labels should be a list so that labels[i] corresponds to the sentence encoded by embeddings[i]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Visualisation with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downside of embeddings like these is that a 384-dimensional embedding space is hard to visualise or interpret.  We need a _dimensionality reduction_ technique like [t-SNE](https://scikit-learn.org/1.5/modules/manifold.html#t-sne) to produce a visualization of vectors in such a space.  This is conveniently implemented by scikit-learn, so we first import the relevant classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b33cc9dd48ee58fa6462ca86f0b8abb",
     "grade": false,
     "grade_id": "cell-84f8f87ddab59d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to **visualise the sentence embeddings from the DBpedia dataset** by using t-SNE to map them into a two-dimensional space, then plot the resulting points as a scatterplot, using the category labels for the _color_ of each point.  This gives you a way to visualise the vectors with respect to the category labels.\n",
    "\n",
    "Complete the function in the cell below, where we have already instantiated the TSNE class.  You can use either plain Matplotlib or Seaborn for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b24f4bdd4cf0d5c3c1bd3cd927c8992b",
     "grade": true,
     "grade_id": "cell-2647656718b47656",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_tsne(vectors, labels, perplexity=30.0, max_iter=1000):\n",
    "    \"\"\"Compute and plot a t-SNE reduction of the given vectors.\n",
    "    \n",
    "    Arguments:\n",
    "        vectors: A list of embedding vectors.\n",
    "        labels: A list of class labels; must have the same length as `vectors`.\n",
    "        perplexity (float): A hyperparameter of the t-SNE algorithm; recommended values\n",
    "            are between 5 and 50, and can result in significantly different results.\n",
    "        max_iter (int): A hyperparameter of the t-SNE algorithm, controlling the maximum\n",
    "            number of iterations of the optimization algorithm.\n",
    "\n",
    "    Returns:\n",
    "        Nothing, but shows the plot.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(verbose=True, perplexity=perplexity, max_iter=max_iter)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "If your implementation is correct, you should be able to produce the plot by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(embeddings, labels, perplexity=30.0, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Afterwards, **re-run the visualisation** a few times with different values for the `perplexity` and `max_iter` parameter, and observe if and how the resulting visualisation changes.  Take a moment to consider how you would interpret the results; you will need this for the reflection part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: CNN/DailyMail\n",
    "\n",
    "In the remainder of this lab, we will look specifically at the task of text summarization. For this, we are using a subset of the CNN/DailyMail&nbsp;3.0.0 data set, a popular dataset for summarization composed of news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49bdc6b118662b49349ec70e8cfcaba6",
     "grade": false,
     "grade_id": "cell-1a94e1bd051732a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with bz2.open(\"cnn_dailymail_3.0.0_shorts.json.bz2\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    news_df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two labelled columns in the data set: `article` (the full news article), and `highlights` (highlights from the article, which we will treat as a “reference summary”).\n",
    "\n",
    "The entire dataset contains 5,000 news articles, but since some of the techniques we will explore here can be quite compute-intensive (depending on the hardware used to run this lab), we limit ourselves to a hand-picked selection of ten articles for some of the exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [6, 53, 56, 340, 730, 1940, 1983, 2404, 2826, 4673]\n",
    "short_news_df = news_df.iloc[indices]\n",
    "short_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Extractive summarization\n",
    "\n",
    "In this problem, we will produce a summary of a news article by extracting a small number of sentences from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Extracting sentence embeddings\n",
    "\n",
    "In the data set, each news article is given as a single string.  Your first task is to **split the text up into sentences** and run them through the sentence embedding model from the Introduction.  You can perform the sentence splitting with the help of spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eff0a6e95e40ca00cf5d4d5da584b669",
     "grade": false,
     "grade_id": "cell-946f1654677db457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "169e447db4715e29447f21b8e8a36bb8",
     "grade": false,
     "grade_id": "cell-ff8bb57e1fe5a1ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentences_and_embeddings(text):\n",
    "    \"\"\"Splits the text into sentences and computes embeddings for each of them.\n",
    "\n",
    "    Arguments:\n",
    "        text: The text to process, e.g. one entire news article.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (sentences, sentence_vectors).  `sentences` should be a list of \n",
    "        sentences from the text, while `sentence_vectors` should be a list of\n",
    "        embedding vectors corresponding to these sentences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell shows how your function should be called, and sanity-checks the returned value for one of the news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975bc87aac828b7070c2152233e99f42",
     "grade": true,
     "grade_id": "cell-24d4917c62190190",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sents, vecs = get_sentences_and_embeddings(short_news_df.iloc[2][\"article\"])\n",
    "\n",
    "# Check if the article appears to be split up correctly\n",
    "assert len(sents) == len(vecs) == 13, \"The news article should produce 13 sentences and 13 vectors\"\n",
    "assert sents[4] == 'The Disney Wonder is registered in the Bahamas.', \"The fifth sentence in the article should be 'The Disney Wonder is registered in the Bahamas.'\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Extractive summarization with MMR\n",
    "\n",
    "You now have all the necessary inputs to produce an extractive summary of a news article. Write a function that takes a list of sentences and their corresponding embedding vectors, as well as a number of sentences to extract from it. Your function should then implement the **maximum marginal relevance (MMR) algorithm** as follows:\n",
    "\n",
    "1. Initially, your candidate set $C$ contains all sentences in the news article, and the set of selected sentences $S$ is empty.\n",
    "2. As the “profile vector” $p$, use the **centroid** of all the sentence vectors from the news article.\n",
    "3. Pick the next sentence to extract from $C\\textbackslash S$ using the marginal relevance formula:\n",
    "\n",
    "$$\n",
    "s_i = \\textrm{arg\\,max}_{s \\in C\\textbackslash S} ~\\left( \\textrm{sim}(s, p) - \\textrm{max}_{s_j \\in S} ~\\textrm{sim}(s, s_j) \\right)\n",
    "$$\n",
    "\n",
    "In this formula, “sim” is the plain **cosine similarity** between the vectors.  We recommend you use the [`cosine_similarity` function from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) for this purpose.\n",
    "\n",
    "4. Repeat step 3 until you have extracted $n$ sentences, where $n$ is given as an argument to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6390805de5cf88a88aeb8354ff419849",
     "grade": true,
     "grade_id": "cell-c5e7eac6c849b557",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def summarize(sentences, vectors, n=3):\n",
    "    \"\"\"Produce an extractive summary from a list of sentences and their vectors.\n",
    "\n",
    "    Arguments:\n",
    "        sentences (list): A list of sentences.\n",
    "        vectors (list): A list of vectors, one for each sentence.\n",
    "        n (int): The number of sentences to extract for the summary.\n",
    "\n",
    "    Returns:\n",
    "        A summary of `n` extracted sentences, as a single string.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell puts it all together: it extracts the sentences and embeddings for one news article, generates the extractive summary, and displays it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = short_news_df.iloc[4]\n",
    "sents, vecs = get_sentences_and_embeddings(news[\"article\"])\n",
    "summary = summarize(sents, vecs, n=3)\n",
    "\n",
    "# Show the article, highlights, and extracted summary in a table\n",
    "pd.DataFrame({\n",
    "    \"article\": [news[\"article\"]],\n",
    "    \"highlights\": [news[\"highlights\"]],\n",
    "    \"summary\": [summary]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation is correct, your summary should look like this:\n",
    "\n",
    "> The Premier League has announced that next season's competition will begin on August 8. The move could allow Roy Hodgson more time to prepare his squad for Euro 2016, if England qualify . The Premier League announced the news on Twitter on Wednesday .\n",
    "\n",
    "Try it out with different news articles to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluating text summarization\n",
    "\n",
    "In this problem, you implement a ROUGE metric to automatically compare the extracted summaries against the “highlights” column of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Implementing ROUGE-2\n",
    "\n",
    "Concretely, you should implement **ROUGE-2**, which is the version of the ROUGE metric based on bigram overlap between a system output and a reference.  In our case, the “system output” is the (concatenated) string of sentences from the generated summary, while the “reference” is the `highlights` column from the news data set.  The ROUGE-2 score is the F1-score computed from the **number of overlapping bigrams** compared against the **total number of bigrams** in the system output and the reference.\n",
    "\n",
    "You will need to tokenize the inputs in order to compute the bigram overlap.  The caveat here is that different tokenizers may result in different ROUGE scores.  For the purpose of this problem, you should use spaCy to tokenize your input.  If in doubt, you can refer to [the spaCy 101 documentation on tokenization](https://spacy.io/usage/spacy-101#annotations-token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe0f46630814a3524ffc1c2c625980ef",
     "grade": false,
     "grade_id": "cell-63aabda8fa7143e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def rouge_2(system, reference):\n",
    "    \"\"\"Compute the ROUGE-2 score between a system output and a reference.\n",
    "    \n",
    "    Arguments:\n",
    "        system (str): The system output, as a single string.\n",
    "        reference (str): The reference to compare against, as a single string.\n",
    "\n",
    "    Returns:\n",
    "        The F1-score of the ROUGE-2 metric between system output and reference.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell tests your implementation of ROUGE-2 with some toy examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687c6101ff610313025a2fc19c1116eb",
     "grade": true,
     "grade_id": "cell-daf48c74916dfc78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert rouge_2(\"System output.\", \"Reference summary.\") == 0.0, \"Two strings without any bigram overlap should return a score of zero\"\n",
    "assert rouge_2(\"Two identical strings.\", \"Two identical strings.\") == 1.0, \"Two identical strings should return a score of one\"\n",
    "assert rouge_2(\"This is my summary.\", \"This is the summary.\") == 0.5, \"In this example, when using spaCy’s tokenization, exactly half of the bigrams overlap, so the ROUGE-2 score should be 0.5\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Baselines for extractive summarization\n",
    "\n",
    "Now that you have an implementation of an evaluation metric, you can use it to evaluate the extractive summaries on the small set of news articles in `short_news_df`.\n",
    "\n",
    "After running the following cell:\n",
    "- `extractive` should contain the extractive summaries for all articles in `short_news_df`, one summary per article, with **two sentences per summary**.\n",
    "- `extractive_rouge_2` should contain the average ROUGE-2 score for the extractive summaries, when evaluated against the \"highlights\" column in `short_news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e139b9bda8f52f780d0cbeaee67d9ad9",
     "grade": true,
     "grade_id": "cell-5a61bde18eb3cd8f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "extractive = []            # Summaries of all articles in `short_news_df`\n",
    "extractive_rouge_2 = ...   # Average ROUGE-2 score\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot say much about the ROUGE-2 score unless we compare it against a **baseline**.  A simple baseline for text summarization is to just take the first $n$&nbsp;sentences from the article.  In the following cell, you should compute the ROUGE-2 score of this baseline, i.e., taking the **first two sentences** of the article as your “summary”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc7d7ccd44592a5381d72bc7ea934a2b",
     "grade": true,
     "grade_id": "cell-5820bf4cd617611b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "baseline_rouge_2 = ...     # Average ROUGE-2 score of baseline\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell prints both ROUGE-2 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5b2a5f20dbfd69e4e9e99904d7fa8d9",
     "grade": false,
     "grade_id": "cell-e0ac74d7536d6e70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ROUGE-2 (baseline)  : {baseline_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (extractive): {extractive_rouge_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Prompting an instruction-tuned LLM\n",
    "\n",
    "In order to perform _abstractive_ summarization, we will turn to an open-source, instruction-fine-tuned large language model (LLM).  Concretely, we will use a quantized version of a [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) model, released in September 2024, that we load via the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) library.  _Quantization_ is a technique to distill a model down to a smaller size, allowing it to run faster and on smaller hardware, while sacrificing some of its performance.\n",
    "\n",
    "The model is loaded in the following cell. Please note:\n",
    "\n",
    "- **If you are running this on the LiU computers,** the following cell will use the model file that we already downloaded to the shared course folder.\n",
    "- **If you are running this on your own computer,** the following cell will automatically download the model from Huggingface the first time you run it, which **requires approx.&nbsp;2&nbsp;GB of disk space!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Path to the model on the LiU computers\n",
    "model_path = \"/courses/TDDE16/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Running on the LiU computers\n",
    "    llm = Llama(model_path, verbose=False)\n",
    "else:\n",
    "    # *NOT* running on the LiU computers\n",
    "    llm = Llama.from_pretrained(\n",
    "        repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
    "    \tfilename=\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",   # 2GB\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "print(f\"Using model: {llm.metadata.get('general.name')}\")\n",
    "print(f\"Stored at: {llm.model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text with this model is simple — we can just call the `llm` object directly.  Note that we don’t have to (and in fact _shouldn’t_) perform any preprocessing on the text input, as this is all done within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"What is the capital of Sweden?\", max_tokens=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the cell above multiple times** and see how the output changes!\n",
    "\n",
    "You should notice two things:\n",
    "1. The call to `llm()` returns more than just the generated text; the generated output is wrapped in a bunch of metadata about the model and the generation.\n",
    "2. The generated text contains more than just the answer to our question, adding more or less relevant (or sensible) output after the answer itself. _(If you haven’t observed that, run the generation a few more times!)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the second issue first.  Instruction-tuned models like Llama are fine-tuned with a specific _chat_ or _prompt template_ that structures the input in a pre-defined way.  For Llama models, this template might look approximately like this:\n",
    "\n",
    "```\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Answer the user query as accurately as possible.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the capital of Sweden?<|eot_id|>\n",
    "```\n",
    "\n",
    "There are different, clearly separated parts in the template, corresponding to different _roles_ of the respective input:\n",
    "\n",
    "- The **system** role contains global instructions for the LLM (e.g. how to behave, or meta information such as its knowledge cutoff date).\n",
    "- The **user** role contains instructions from the user.\n",
    "- The **input** role (not shown above) contains additional _input data_ that the LLM can refer to when processing the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The llama-cpp-python library provides a function for automatically applying the correct template for the loaded model; it takes a list of _messages_, which are text strings with their corresponding roles.  Here’s an example of how to call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.create_chat_completion(\n",
    "\tmessages=[\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": \"What is the capital of Sweden?\"\n",
    "\t\t},\n",
    "\t],\n",
    "    max_tokens=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the output now only contains the answer to the question, and no further chatter! You should also see that, in contrast to simply calling `llm()`, using the prompt template makes the model response appear in a message with the **assistant** role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Understanding prompt templates\n",
    "\n",
    "Your task now is to test the behaviour of prompt templates (and your understanding of them) with the following exercise:\n",
    "\n",
    "**Write a function that returns the capital of a country by prompting the LLM.**  In particular, your function should:\n",
    "\n",
    "- use the \"system\" message to make the LLM return only the desired answer, rather than a full sentence (i.e., not _\"The capital of Sweden is Stockholm.\"_ but just _\"Stockholm\"_)\n",
    "- use the \"input\" message to supply the name of the country\n",
    "- return only a text string with the generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87e754cae5a15c3db10aa155af2f9409",
     "grade": true,
     "grade_id": "cell-e5c606393991af24",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_capital_of(country):\n",
    "    \"\"\"Prompt an LLM for the capital of a country.\n",
    "    \n",
    "    Arguments:\n",
    "        country (str): The name of the country.\n",
    "\n",
    "    Returns:\n",
    "        A text string containing the LLM’s response.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell should return only “Stockholm”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06677ad9f6289e7cd7a55149f2574210",
     "grade": false,
     "grade_id": "cell-8689c414b0d9329b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "get_capital_of(\"Sweden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code works with the input “Sweden”, **re-run the function with different inputs** to see what happens.  Does the LLM return the correct answer even for lesser-known countries?  What happens if you give it input that’s not an actual country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example — try out your own inputs!\n",
    "get_capital_of(\"Iceland\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Abstractive summarization\n",
    "\n",
    "We now have everything we need to perform abstractive summarization, by prompting our LLM to generate a summary based on the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Prompting for summaries\n",
    "\n",
    "In the same vein as in Task 4.1, **write a function that returns a summary of the input text** by prompting the LLM.  Note the following:\n",
    "\n",
    "- Since we could specify the desired number of sentences for the extractive summaries (in Task&nbsp;2.2), we want to have the same functionality here, so construct your messages in a way that request the given number of sentences from the LLM.  (Note that this is not an absolute guarantee that the LLM will always follow this instruction, but it should do so in most cases.)\n",
    "- As before, construct your messages in such a way that the returned response will only contain the requested summary, without any further text (such as “Sure, here is a summary...”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cebbe4eb890fa6725a6fd20fac0a7747",
     "grade": true,
     "grade_id": "cell-e0a6ad3731e55108",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def llm_summarize(text, n=3):\n",
    "    \"\"\"Produce an abtractive summary for a given text.\n",
    "    \n",
    "    Arguments:\n",
    "        text (str): The text to summarize.\n",
    "        n (int): The number of sentences to extract for the summary.\n",
    "\n",
    "    Returns:\n",
    "        A text string containing the abstractive summary produced by the LLM.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell should produce a summary for one of the news articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = short_news_df.iloc[4]\n",
    "summary = llm_summarize(news[\"article\"], n=3)\n",
    "\n",
    "# Show the article, highlights, and extracted summary in a table\n",
    "pd.DataFrame({\n",
    "    \"article\": [news[\"article\"]],\n",
    "    \"highlights\": [news[\"highlights\"]],\n",
    "    \"summary\": [summary]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Generate all summaries & evaluate\n",
    "\n",
    "The only thing left to do is to generate summaries for all the articles in our (tiny) dataset, compute their ROUGE-2 scores against the \"highlights\" column, and compare the results against both the baseline and the extractive summaries.\n",
    "\n",
    "After running the following cell:\n",
    "- `abstractive` should contain the abstractive summaries for all articles in `short_news_df`, one summary per article.\n",
    "  -  **Important:** Since we used two sentences for the extractive summarization & baselines, we should also prompt for **two-sentence summaries** here.\n",
    "- `abstractive_rouge_2` should contain the average ROUGE-2 score for the abstractive summaries, when evaluated against the \"highlights\" column.\n",
    "\n",
    "**_Note:_** This cell should take the longest time to run out of all exercises in this notebook; in our testing, it took around 3–5 minutes on the lab computers.  You may want to [use a progress bar](https://rich.readthedocs.io/en/stable/progress.html) or print an update in each loop iteration to get an idea of how many articles have already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa3d8805573d45e7d43c5c78aeedeed0",
     "grade": true,
     "grade_id": "cell-9960c7a1f1c1d578",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "abstractive = []             # Summaries of all articles in `short_news_df`\n",
    "abstractive_rouge_2 = ...    # Average ROUGE-2 score\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤞 Test your code\n",
    "\n",
    "The following cell prints all ROUGE-2 scores, and the cell after that displays a table with the highlights and generated summaries, side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03d19a87db7a42466c7a2f3d8469db72",
     "grade": false,
     "grade_id": "cell-5913076d05206107",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"ROUGE-2 (baseline)   : {baseline_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (extractive) : {extractive_rouge_2:.4f}\")\n",
    "print(f\"ROUGE-2 (abstractive): {abstractive_rouge_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\n",
    "    \"highlights\": short_news_df.loc[:, \"highlights\"],\n",
    "    \"extractive\": extractive,\n",
    "    \"abstractive\": abstractive\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the summaries (and therefore the ROUGE scores) will most likely be different if you run the same prompt again, so we encourage you to run it at least twice before you write up your reflection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual reflection\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>After you have solved the lab,</strong> write a <em>brief</em> reflection (max. one A4 page) on the question(s) below.  Remember:\n",
    "    <ul>\n",
    "        <li>You are encouraged to discuss this part with your lab partner, but you should each write up your reflection <strong>individually</strong>.</li>\n",
    "        <li><strong>Do not put your answers in the notebook</strong>; upload them in the separate submission opportunity for the reflections on Lisam.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Problem&nbsp;1, you visualised sentence embeddings with the help of t-SNE.  How do you interpret the results you got?  Were there any patterns, and/or did the parameters of t-SNE make a difference?\n",
    "2. Pick one example from the final evaluation in Task&nbsp;5.2 and discuss what qualitative differences you see between the extractive and abstractive summaries.  Do the ROUGE-2 scores you got agree with your own judgment on how “good” the summaries are?  What conclusion do you draw from this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! 👍**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "➡️ Before you submit, **make sure the notebook can be run from start to finish** without errors.  For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"⏩\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad192d-7557-4cd9-9ead-6699b8de9114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
