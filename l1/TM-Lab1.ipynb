{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccd4d3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you start, make sure that you are familiar with the **[study guide](https://liu-nlp.ai/text-mining/logistics/)**, in particular the rules around **cheating and plagiarism** (found in the course memo).\n",
    "\n",
    "‚û°Ô∏è If you use code from external sources (e.g. StackOverflow, ChatGPT, ...) as part of your solutions, don't forget to add a reference to these source(s) (for example as a comment above your code).\n",
    "\n",
    "‚û°Ô∏è Make sure you fill in all cells that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**.  You normally shouldn't need to modify any of the other cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4f84c-26c4-4bcb-81d1-8757088f0623",
   "metadata": {},
   "source": [
    "# L1: Information Retrieval\n",
    "\n",
    "In this lab you will apply basic techniques from information retrieval to implement the core of a minimalistic search engine. The data for this lab consists of a collection of app descriptions scraped from the [Google Play Store](https://play.google.com/store/apps?hl=en). From this collection, your search engine should retrieve those apps whose descriptions best match a given query under the vector space model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92aa93-cf15-4e1c-975e-fea9bbe0b0c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cbe3dc2eff09907e453d02296e6f2a3",
     "grade": false,
     "grade_id": "cell-f766ed4c371f7a04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define some helper functions that are used in this notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def success():\n",
    "    display(HTML('<div class=\"alert alert-success\"><strong>Checks have passed!</strong></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5345b-0f8f-4a58-b7d3-bd5baae0c281",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a82b5c-1660-4389-942b-f15420289549",
   "metadata": {},
   "source": [
    "The app descriptions come in the form of a compressed [JSON](https://en.wikipedia.org/wiki/JSON) file. Start by loading this file into a [Pandas](https://pandas.pydata.org) [DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4982e3-3df4-4837-97b5-4c300b0d4a20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f1e59d0ec4fbd656e2335705c753302",
     "grade": false,
     "grade_id": "cell-c5ac0bec64889197",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "with bz2.open('app-descriptions.json.bz2', mode='rt', encoding='utf-8') as source:\n",
    "    df = pd.read_json(source, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30823068-3102-430d-9989-b4f530756a08",
   "metadata": {},
   "source": [
    "In Pandas, a DataFrame is a table with indexed rows and labelled columns of potentially different types. You can access data in a DataFrame in various ways, including by row and column. To give an example, the code in the next cell shows rows 200‚Äì204:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b8fbb-7a52-4cec-8d9f-921f4694e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[200:205]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27186bc1-de90-4125-837a-7b4e8671d276",
   "metadata": {},
   "source": [
    "As you can see, there are two labelled columns: `name` (the name of the app) and `description` (a textual description). The next cell shows how to access only the description field from row 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595bb7af-5ee4-4b8e-8f5e-df5aaae688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[200, 'description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9d0cb-a8bb-4e94-9b8c-b6b33651ac8e",
   "metadata": {},
   "source": [
    "## Problem 1: What's in a vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a37f90-e7d8-4542-89e0-bea664601769",
   "metadata": {},
   "source": [
    "We start by vectorising the data ‚Äî more specifically, we map each app description to a tf‚Äìidf vector. This is very simple with a library like [scikit-learn](https://scikit-learn.org/stable/), which provides a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class for exactly this purpose.  If we instantiate this class, and call `fit_transform()` on all of our app descriptions, scikit-learn will preprocess and tokenize each app description, compute tf‚Äìidf values for each of them, and return a vectorised representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca674d8-c2df-4c8f-bb3d-6d59bdc401fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b537a1d38dc3fae5409caa6d374611d",
     "grade": false,
     "grade_id": "cell-eeff6351582552c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['description'])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2af7b6-e140-45ec-ba15-3428b089e283",
   "metadata": {},
   "source": [
    "Let‚Äôs pick the app \"Pancake Tower\", which has a rather short description text, to see how it has been vectorised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d112e7-e564-4e47-a633-38910709947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use 'toarray' to convert the sparse matrix object into a \"normal\" array\n",
    "vec = X[1032].toarray()[0]\n",
    "\n",
    "# The app description & its corresponding vector\n",
    "df.loc[1032, 'description'], vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778f9cb-20ff-44b2-939b-cbf6b343558d",
   "metadata": {},
   "source": [
    "That's not very informative yet.  We know that the vector contains tf‚Äìidf values, and that each dimension of the vector corresponds to a token in the vectorizer‚Äôs vocabulary; let's extract these for this specific example.\n",
    "\n",
    "Your **first task** is to find out how to access the `vectorizer`‚Äôs vocabulary, for example by [checking the documentation of `TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), and print all the tokens that are represented in the vector with a tf‚Äìidf value greater than zero (i.e., only the tokens that are actually part of this app‚Äôs description) _in descending order of the tf‚Äìidf values_.  In other words, the token with the highest tf‚Äìidf value should be at the top of your output, and the token with the lowest tf‚Äìidf value at the bottom.   Before you implement this, think about what you would expect the output look like, for example which words you would expect to have the highest/lowest tf‚Äìidf values in this example.\n",
    "\n",
    "Your final output should look something like this:\n",
    "\n",
    "```\n",
    "<token 1>: <tf-idf value 1>\n",
    "<token 2>: <tf-idf value 2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43ad18-aa4e-4d31-85de-0a7419767c37",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef45e05c7a2274510589bc9317ef0096",
     "grade": true,
     "grade_id": "cell-95a3350b5c599c39",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Print the tokens and their tf‚Äìidf values, in descending order.\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa657ba6-4845-4569-9df5-0d7f4db26a2d",
   "metadata": {},
   "source": [
    "## Problem 2: Finding the nearest vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca7f1a-904b-4b1f-8ed2-8e5688603414",
   "metadata": {},
   "source": [
    "To build a small search engine, we need to be able to turn _queries_ (for example the string \"pile up pancakes\") into _query vectors_, and then find out which of our app description vectors are closest to the query vector.\n",
    "\n",
    "For the first part (turning queries into query vectors), we can simply re-use the `vectorizer` that we used for the app descriptions. For the second part, an easy way to find the closest vectors is to use scikit-learn‚Äôs [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) class. This class needs to be _fit_ on a set of vectors (the \"training set\"; in our case the app descriptions) and can then be used with any vector to find its _nearest neighbors_ in the vector space.\n",
    "\n",
    "**First,** instantiate and fit a class that returns the _ten (10)_ nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf46d50-ea74-42d7-86e2-6520dde6eac6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfdd47cca90ffba7c29bbd4436e06dde",
     "grade": false,
     "grade_id": "cell-bcebb6d4a62e7b7f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Instantiate and fit a class that returns the 10 nearest neighboring vectors.\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79b405-a236-4caf-a140-dff01e0ee88b",
   "metadata": {},
   "source": [
    "**Second,** implement a function that uses the vectorizer and the fitted class to find the nearest neighbours for a given query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bfb5a-3648-44ad-aa3a-dfa39838ffc3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7691715bfd90372d8e5a7e05f9f463f5",
     "grade": false,
     "grade_id": "cell-18bf238a099d709b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    \"\"\"Find the nearest neighbors in `df` for a query string.\n",
    "\n",
    "    Arguments:\n",
    "      query (str): A query string.\n",
    "\n",
    "    Returns:\n",
    "      The 10 apps (with name and description) most similar (in terms of\n",
    "      cosine similarity) to the given query as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbb9f7-1cf5-49b9-a364-7b23fb6bbd1d",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following cell, which will sanity-check your return value and show the 10 best search results for the query _\"pile up pancakes\"_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30a52e-bdac-412b-b9c4-8e9cc17436a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ee8bcf553647fa7ad499be8f7631289",
     "grade": true,
     "grade_id": "cell-8506d7fe5961f87b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that searching for \"pile up pancakes\" returns a DataFrame with ten results,\n",
    "   and that the top result is \"Pancake Tower\".\"\"\"\n",
    "\n",
    "result = search('pile up pancakes')\n",
    "display(result)\n",
    "assert isinstance(result, pd.DataFrame), \"search() function should return a Pandas DataFrame\"\n",
    "assert len(result) == 10, \"search() function should return 10 search results\"\n",
    "assert result.iloc[0][\"name\"] == \"Pancake Tower\", \"Top search result should be 'Pancake Tower'\"\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27802387-4fe0-4e3f-bb4e-c0ef048ca721",
   "metadata": {},
   "source": [
    "Before continuing with the next problem, play around a bit with this simple search functionality by trying out different search queries, and see if the results look like what you would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d95fe-cfe3-4225-8c8e-2e9496c6eea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example ‚Äî try out your own queries!\n",
    "search(\"dodge trains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb8872-b7bc-4f7e-a01f-6f764c2c6258",
   "metadata": {},
   "source": [
    "## Problem 3: Custom preprocessing & tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a34a04-425e-4067-bcf3-00d2f3edc727",
   "metadata": {},
   "source": [
    "In Problem 1, you should have seen that `TfidfVectorizer` already performs some preprocessing by default and also does its own tokenization of the input data. This is great for getting started, but often we want to have more control over these steps. We can customize some aspects of the preprocessing through arguments when instantiating `TfidfVectorizer`, but for this exercise, we want to do _all_ of our preprocessing & tokenizing outside of scikit-learn.\n",
    "\n",
    "Concretely, we want to use [spaCy](https://spacy.io), a library that we will make use of in later labs as well.  Here is a brief example of how to load and use a spaCy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dadb46-66b7-4d53-a9db-48849c6cb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the small English model, disabling some components that we don't need right now\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'])\n",
    "\n",
    "# Take an example sentence and print every token from it separately\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a9865-f314-4d80-9fac-e6544413ee3a",
   "metadata": {},
   "source": [
    "**Your task** is to write a preprocessing function that uses spaCy to perform the following steps:\n",
    "- tokenization\n",
    "- lemmatization\n",
    "- stop word removal\n",
    "- removing tokens containing non-alphabetical characters\n",
    "\n",
    "We recommend that you go through the [Linguistic annotations](https://spacy.io/usage/spacy-101#annotations) section of the spaCy&nbsp;101, which demonstrates how you can get the relevant kind of information via the spaCy library.\n",
    "\n",
    "Implement your preprocessor by completing the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a6fc6-dee8-4140-bf7a-1a245e60a1b3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1c51dc3d0c0c8ba1517e852d7d5df1e",
     "grade": false,
     "grade_id": "cell-2df4eac94cdf6be3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocess the given text by tokenising it, removing any stop words, \n",
    "    replacing each remaining token with its lemma (base form), and discarding \n",
    "    all lemmas that contain non-alphabetical characters.\n",
    "\n",
    "    Arguments:\n",
    "      text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "      The list of remaining lemmas after preprocessing (represented as strings).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af08b57-8e0c-4526-b7fc-94c8bace1936",
   "metadata": {},
   "source": [
    "### ü§û Test your code\n",
    "\n",
    "Test your implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918affb-f8aa-4cbf-9cbe-b03b9251e941",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "539b9043be91aac1f9d82fb62dea6112",
     "grade": true,
     "grade_id": "cell-642185b139f2cef6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that the preprocessing returns the correct output for a number of test cases.\"\"\"\n",
    "\n",
    "assert (\n",
    "    preprocess('Apple is looking at buying U.K. startup for $1 billion') ==\n",
    "    ['Apple', 'look', 'buy', 'startup', 'billion']\n",
    ")\n",
    "assert (\n",
    "    preprocess('\"Love Story\" is a country pop song written and sung by Taylor Swift.') ==\n",
    "    ['Love', 'Story', 'country', 'pop', 'song', 'write', 'sing', 'Taylor', 'Swift']\n",
    ")\n",
    "success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62063fdc-86bc-48cb-b045-04308358f852",
   "metadata": {},
   "source": [
    "## Problem 4: The effect of preprocessing\n",
    "\n",
    "To make use of the new `preprocess` function from Problem 3, we need to make sure that we incorporate it into `TfidfVectorizer` and disable all preprocessing & tokenization that `TfidfVectorizer` performs by default. Afterwards, we also need to re-fit the vectorizer and the nearest-neighbors class. To make this a bit easier to handle, let‚Äôs take everything we have done so far and put it in a single class `AppSearcher`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb215-dd45-40aa-9c36-d6fb37aa6d28",
   "metadata": {},
   "source": [
    "### Task 4.1\n",
    "\n",
    "**Your first task** is to complete the stub of the `AppSearcher` class given below. Keep in mind:\n",
    "- The `fit()` function should fit both the vectorizer (from Problem 1) and the nearest-neighbors class (from Problem 2).  Make sure to modify the call to `TfidfVectorizer` to _disable all preprocessing & tokenization_ that it would do by default, and replace it with a call to the `preprocess()` function _defined in `AppSearcher`_.\n",
    "- For the `preprocess()` function, you can start by copying your solution from Problem 3.\n",
    "- For the `search()` function, you can copy your solution from Problem 2.\n",
    "- Make sure to adapt your code to store the everything (data, vectorizer, nearest-neighbors class) within the `AppSearcher` class, so that your solution is independent of the code you wrote above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66665d-24da-4506-9f4f-c1427caa039e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "344165098a156da25b2adc3e9e70168b",
     "grade": true,
     "grade_id": "cell-cb53b61e9efe98af",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AppSearcher:\n",
    "    def fit(self, df):\n",
    "        \"\"\"Instantiate and fit all the classes required for the search engine (cf. Problems 1 and 2).\"\"\"\n",
    "        self.df = df\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess the given text (cf. Problem 3).\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"Find the nearest neighbors in `df` for a query string (cf. Problem 2).\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ca3f6-c086-4588-8e17-4558cd7ebe99",
   "metadata": {},
   "source": [
    "#### ü§û Test your code\n",
    "\n",
    "The following cell demonstrates how your class should be used. Note that it can take a bit longer to train it on the data as before, since we‚Äôre now calling spaCy for the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d42be-4e31-44c7-b1e4-afe3dc77a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = AppSearcher()\n",
    "apps.fit(df)\n",
    "apps.search(\"pile up pancakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adce322-80ee-44b9-bd7f-5c910a90330e",
   "metadata": {},
   "source": [
    "### Task 4.2\n",
    "\n",
    "**Your second task** is to experiment with the effect of using (or not using) different preprocessing steps.  We always need to _tokenize_ the text, but other preprocessing steps are optional and require a conscious decision whether to use them or not, such as:\n",
    "- lemmatization\n",
    "- lowercasing all characters\n",
    "- removing stop words\n",
    "- removing tokens containing non-alphabetical characters\n",
    "\n",
    "**Modify the definition of the `preprocess()` function** of `AppSearcher` to include/exclude individual preprocessing steps, run some searches, and observe if and how the results change.  Which search queries you try out is up to you ‚Äî you could compare searching for \"pile up pancakes\" with \"pancake piling\", for example; or you could try entirely different search queries aimed at different kinds of apps.  (You can modify the class directly by changing the cell above under Task 4.1, or copy the definitions to the cells below, whichever you prefer; there is no separate code to show for this task, but you will use your observations here for the individual reflection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59c86d-2c98-4d1b-a267-89aa13871cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de6f8ce3-d22d-47c1-9f8c-5f1b8b7ab027",
   "metadata": {},
   "source": [
    "## Individual reflection\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>After you have solved the lab,</strong> write a <em>brief</em> reflection (max. one A4 page) on the question(s) below.  Remember:\n",
    "    <ul>\n",
    "        <li>You are encouraged to discuss this part with your lab partner, but you should each write up your reflection <strong>individually</strong>.</li>\n",
    "        <li><strong>Do not put your answers in the notebook</strong>; upload them in the separate submission opportunity for the reflections on Lisam.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b377f-64eb-475b-94c9-d8bd27a68039",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. In Problem 1, which token had the highest tf‚Äìidf score, which the lowest?  Based on your knowledge of how tf‚Äìidf works, how would you explain this result?\n",
    "2. Based on your observations in Problem 4, which preprocessing steps do you think are the most appropriate for this \"search engine\" example?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! üëç**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you submit, **make sure the notebook can be run from start to finish** without errors.  For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"‚è©\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad192d-7557-4cd9-9ead-6699b8de9114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
